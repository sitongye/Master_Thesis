{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 126] 找不到指定的模块。",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ee0a75ed39a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[0mdlls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mth_dll_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'*.dll'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdll\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdlls\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdll\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\ctypes\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] 找不到指定的模块。"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.displacy import render,serve\n",
    "from IPython.core.display import display, HTML\n",
    "from spacy.tokens import Span\n",
    "from spacy.displacy import render,serve\n",
    "import networkx as nx\n",
    "import itertools\n",
    "import json\n",
    "import pylab\n",
    "from spacy.util import filter_spans\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from transformers import *\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Lambda, Dense, Concatenate\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for base model\n",
    "# load libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_col2feat(train_data,test_data,column,paradict,bow=True):\n",
    "    features_train, features_test,vectorizer = bag_of_word(train_data[column].apply(lambda x: \" \".join(x)),\n",
    "                                                            test_data[column].apply(lambda x: \" \".join(x)),\n",
    "                                                            paradict,bow=bow)\n",
    "    return features_train, features_test,vectorizer\n",
    "\n",
    "def bow_train_feature_stack(train_data,test_data,paradict,column_list,bow=True):\n",
    "    features_stack_train = None\n",
    "    features_stack_test = None\n",
    "    for column in column_list:\n",
    "        features_train,features_test,_ = bow_col2feat(train_data,test_data,column,paradict,bow=bow)\n",
    "        features_stack_train = hstack((features_stack_train, features_train))\n",
    "        features_stack_test = hstack((features_stack_test,features_test))\n",
    "    return features_stack_train, features_stack_test\n",
    "\n",
    "def bag_of_word(train,test,paradict,bow=True):\n",
    "    vectorizer = CountVectorizer(ngram_range=paradict[\"NGRAM_RANGE\"],\n",
    "                                 max_features=paradict[\"MAX_FEATURES\"],\n",
    "                                 binary=paradict[\"BINARY\"],\n",
    "                                 max_df=paradict[\"MAX_DF\"])\n",
    "    if bow==False:\n",
    "        vectorizer = TfidfVectorizer(ngram_range=paradict[\"NGRAM_RANGE\"],\n",
    "                                 max_features=paradict[\"MAX_FEATURES\"],\n",
    "                                 binary=paradict[\"BINARY\"],\n",
    "                                 max_df=paradict[\"MAX_DF\"])\n",
    "    train_data_features = vectorizer.fit_transform(train)\n",
    "    test_data_features = vectorizer.transform(test)\n",
    "    return train_data_features, test_data_features, vectorizer\n",
    "\n",
    "bow_paradict = {\n",
    "\"NGRAM_RANGE\":(1,3),\n",
    "\"MAX_FEATURES\": 30,\n",
    "\"BINARY\": False,\n",
    "\"MAX_DF\":0.95}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BertMerger import BadTERMMerger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELED_DATA = r\"..\\data\\processed_data\\labeled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(r\".\\ner\\parameter\")\n",
    "nlp.add_pipe(nlp.create_pipe('merge_entities'))\n",
    "term_merger = BadTERMMerger(nlp)\n",
    "nlp.add_pipe(term_merger, before=\"ner\")\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_quant_token(row):\n",
    "    q_t = None\n",
    "    i = None\n",
    "    text_doc = nlp(row[\"Text\"])\n",
    "    for t in text_doc:\n",
    "        if row[\"Quant\"].strip().lower() in t.text.lower():\n",
    "            q_t = t\n",
    "            i = t.i\n",
    "    return q_t, i\n",
    "\n",
    "def return_target_token(row):\n",
    "    t_t = None\n",
    "    i = None\n",
    "    text_doc = nlp(row[\"Text\"])\n",
    "    for t in text_doc:\n",
    "        if str(row[\"target\"]).strip().lower() in t.text.lower():\n",
    "            t_t = t\n",
    "            i = t.i\n",
    "    return t_t, i\n",
    "\n",
    "def sent_snippet(row):\n",
    "    span = None\n",
    "    if (row[\"Quant_Token\"][1] is not None) and (row[\"target_Token\"][1] is not None):\n",
    "        start = min(row[\"Quant_Token\"][1],row[\"target_Token\"][1])\n",
    "        end = max(row[\"Quant_Token\"][1],row[\"target_Token\"][1])\n",
    "        span = row[\"Tokens\"][start:end+1]\n",
    "    return span\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_labeled_data(data_path):\n",
    "    gesamt = pd.DataFrame()\n",
    "    for path, _, files in os.walk(data_path):\n",
    "        for FILE_NAME in tqdm(files):\n",
    "            try:\n",
    "                test_input = pd.read_excel(os.path.join(path, FILE_NAME),usecols = [\"Text\",\"Quant\",\"Target\",\"Label\"],dtype={\"Text\":str,\"Quant\": str,\"Target\":str})\n",
    "                test_input.dropna(subset=[\"Label\"], inplace=True,axis=0)\n",
    "                test_input = test_input.reset_index(drop=True)\n",
    "                test_input.rename(columns={\"Target\":\"target\",\"Label\":\"label\"}, inplace=True)\n",
    "                test_input[\"label\"] = test_input[\"label\"].apply(lambda x: int(x))\n",
    "                test_input[\"Tokens\"] = test_input.Text.apply(lambda x: [t for t in nlp(x)])\n",
    "                test_input[\"Quant_Token\"] = test_input.apply(return_quant_token, axis=1)\n",
    "                test_input[\"target_Token\"] = test_input.apply(return_target_token, axis=1)\n",
    "                test_input[\"Sent_Span\"] = test_input.apply(sent_snippet, axis=1)\n",
    "                valided_index = []\n",
    "                for row in range(len(test_input)):\n",
    "                    tokens = test_input.loc[row, \"Tokens\"]\n",
    "                    quant_index= test_input.loc[row, \"Quant_Token\"][1]\n",
    "                    target_index = test_input.loc[row, \"target_Token\"][1]\n",
    "                    if quant_index is not None:\n",
    "                        if tokens[quant_index].head==tokens[target_index].head:\n",
    "                            valided_index.append(row)\n",
    "                cleaned = test_input.loc[valided_index,:]\n",
    "                cleaned =test_input.reset_index(drop=True)\n",
    "                print(FILE_NAME)\n",
    "                gesamt = gesamt.append(cleaned,ignore_index=True)\n",
    "            except:\n",
    "                print(\"ERROR!:\",FILE_NAME)\n",
    "                continue\n",
    "    return gesamt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_labeled_data(data_path=LABELED_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_df = train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = tokenized_df.groupby(by=\"label\").count().reset_index()\n",
    "num2label = {0: \"NoRelation\",\n",
    "             1: \"MinValue\",\n",
    "             2: \"MaxValue\",\n",
    "             3: \"IsValue\"}\n",
    "count[\"label\"] = count[\"label\"].apply(lambda x: num2label[x])\n",
    "bar = count.plot(kind=\"bar\",x=\"label\", y=\"Text\",rot=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"The mean wind speed measured at a height of at least 1 m above the ground shall be less than 6 m/s with gusts not exceeding 10 m/s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc,style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "#bert_model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_token_pre(dataframe, index, output = \"all\"):\n",
    "    \"\"\"\n",
    "    output: \"all\"- output whole sentence\n",
    "    output: \"sub\" - output separately \n",
    "    \"\"\"\n",
    "    end = max(dataframe.loc[index, \"Quant_Token\"][1],dataframe.loc[index, \"target_Token\"][1])\n",
    "    start = min(dataframe.loc[index, \"Quant_Token\"][1],dataframe.loc[index, \"target_Token\"][1])\n",
    "    span_before = \" \".join([t.text for t in dataframe.loc[index,\"Tokens\"][:start-1]])\n",
    "    span_current = \" \".join([t.text for t in dataframe.loc[index, \"Tokens\"][start:end+1]])\n",
    "    span_after = \" \".join([t.text for t in dataframe.loc[index, \"Tokens\"][end+1:]])\n",
    "    ber_tok_before = tokenizer.tokenize(span_before)\n",
    "    ber_tok_span = tokenizer.tokenize(span_current)\n",
    "    ber_tok_after = tokenizer.tokenize(span_after)\n",
    "    ber_tok_all = ber_tok_before + ber_tok_span + ber_tok_after\n",
    "    if output == \"all\":\n",
    "        return ber_tok_all\n",
    "    elif output == \"sub\":\n",
    "        return ber_tok_before, ber_tok_span, ber_tok_after"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "whole_sent = bert_token_pre(tokenized_df, 0, output = \"all\")\n",
    "bf, main, af = bert_token_pre(tokenized_df,0,output = \"sub\")\n",
    "whole_sent[len(bf):-len(af)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-8d4404c0b499>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# well let's first get out all the bert embedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtokenized_df_bert\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenized_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtokenized_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSent_Span\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtokenized_df_bert\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenized_df_bert\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtokenized_df_bert\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQuant_Token\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtokenized_df_bert\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenized_df_bert\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtokenized_df_bert\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_Token\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtokenized_df_bert\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenized_df_bert\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenized_df' is not defined"
     ]
    }
   ],
   "source": [
    "# well let's first get out all the bert embedding\n",
    "tokenized_df_bert = tokenized_df[tokenized_df.Sent_Span!=None]\n",
    "tokenized_df_bert = tokenized_df_bert[tokenized_df_bert.Quant_Token != (None,None)]\n",
    "tokenized_df_bert = tokenized_df_bert[tokenized_df_bert.target_Token!=(None,None)]\n",
    "tokenized_df_bert = tokenized_df_bert.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenized_df_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXTEXTSPAN=max(tokenized_df_bert[\"Sent_Span\"].apply(lambda x: len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.encode_plus(tokenized_df.loc[0,\"Text\"],)\n",
    "def padding(tokenized_list, max_length):\n",
    "    if len(tokenized_list) < max_length:\n",
    "        tokens = [\"[CLS]\"] + tokenized_list + [\"[SEP]\"] + (max_length - len(tokenized_list))*[\"[PAD]\"]\n",
    "    else:\n",
    "        tokens = [\"[CLS]\"] + tokenized_list + [\"[SEP]\"]\n",
    "    return tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. add [CLS] [SEP] tokens\n",
    "def output_maxlen(dataframe):\n",
    "    maxlen = 0\n",
    "    maxlen = max([len(tokenizer.tokenize(text)) for text in dataframe[\"Text\"]])\n",
    "    return maxlen\n",
    "MAX_LEN = output_maxlen(tokenized_df)\n",
    "MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for input_id\n",
    "def get_hidden_state(dataframe):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for index in range(len(dataframe)):\n",
    "        tokenized_list = bert_token_pre(dataframe, index, output = \"all\")\n",
    "        padded = padding(tokenized_list, output_maxlen(dataframe))\n",
    "        attention_mask = [1 if t!=\"[PAD]\" else 0 for t in padded]\n",
    "        encoded = tokenizer.encode_plus(padded, add_special_tokens=False, pad_to_max_length=True,\n",
    "                                       is_pretokenized = True, return_tensors=\"pt\")\n",
    "        input_id = encoded[\"input_ids\"]\n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(torch.tensor(attention_mask))\n",
    "    print(len(input_ids))\n",
    "    print(len(attention_masks))\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    ats = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids=input_ids)[0].numpy()\n",
    "    return last_hidden_states\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_data(train_df, last_hidden_states_np):\n",
    "    length_bert_tokens = []\n",
    "    lstm_inputs = []\n",
    "    for i in range(len(train_df)):\n",
    "        bf, span, after = bert_token_pre(train_df, i, output = \"sub\")\n",
    "        all_bert_tokens = bert_token_pre(train_df, i, output = \"all\")\n",
    "        bert_index = (len(bf),len(after))\n",
    "        #print(bert_index)\n",
    "        length_bert_tokens.append(len(all_bert_tokens[len(bf):len(bf)+len(span)]))\n",
    "        bert_span_start = len(bf)+1\n",
    "        bert_span_end = len(bf)+len(span)\n",
    "        hidden_state = last_hidden_states_np[i,len(bf):len(bf)+len(span),:]\n",
    "        lstm_inputs.append(hidden_state)\n",
    "    padded = pad_sequences(lstm_inputs, maxlen=MAXTEXTSPAN, padding='post')\n",
    "    return padded\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(set(tokenized_df_bert[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEN_LEN = MAXTEXTSPAN\n",
    "DIM = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEN_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bert_input = Input(shape=(SEN_LEN,DIM))\n",
    "X, X_lstm1, _ = LSTM(768, dropout=0.3, return_state=True, return_sequences=True)(Bert_input)\n",
    "X, X_lstm2, _ = LSTM(768, dropout=0.3, go_backwards=True, return_state=True)(X)\n",
    "X = Concatenate()([X_lstm1,X_lstm2])\n",
    "X= Dense(4, activation=\"softmax\")(X)\n",
    "\n",
    "general_model = Model(Bert_input,X, name=\"bertforRX\")\n",
    "general_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "general_model.compile(Adam(5e-5), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "num2label = {0:\"NoRelation\",\n",
    "             1:\"MinValue\",\n",
    "             2: \"MaxValue\",\n",
    "             3: \"IsValue\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tokenized_df_bert.loc[:,~tokenized_df_bert.columns.isin([\"label\"])], tokenized_df_bert[\"label\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"span\"] = X_train[\"Sent_Span\"].apply(lambda x: [k.text for k in x])\n",
    "X_test[\"span\"] = X_test[\"Sent_Span\"].apply(lambda x:[k.text for k in x])\n",
    "X_train[\"pos\"] = X_train[\"Sent_Span\"].apply(lambda x: [k.pos_ for k in x])\n",
    "X_test[\"pos\"] = X_test[\"Sent_Span\"].apply(lambda x: [k.pos_ for k in x])\n",
    "X_train[\"dep\"] = X_train[\"Sent_Span\"].apply(lambda x: [k.dep_ for k in x])\n",
    "X_test[\"dep\"] = X_test[\"Sent_Span\"].apply(lambda x: [k.dep_ for k in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [X_train, X_test, y_train, y_test]:\n",
    "    df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_stack_train, features_stack_test = bow_train_feature_stack(X_train, X_test, bow_paradict,\n",
    "                                                                    [\"span\",\"pos\", \"dep\"],bow=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=45,solver=\"lbfgs\").fit(features_stack_train,y_train)\n",
    "y_pred=lr.predict(features_stack_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(features_stack_train,y_train)\n",
    "y_pred = svclassifier.predict(features_stack_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## end of base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare train and test data\n",
    "train_last_hidden_states_np = get_hidden_state(X_train)\n",
    "X_train_input = generate_train_data(X_train, train_last_hidden_states_np)\n",
    "test_last_hidden_states_np = get_hidden_state(X_test)\n",
    "X_test_input = generate_train_data(X_test, test_last_hidden_states_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "history = general_model.fit(x=X_train_input,\n",
    "                  y=to_categorical(y_train),batch_size = batch_size,validation_data=(X_test_input,to_categorical(y_test)),\n",
    "                  epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_model.save('bert_relation_extraction')\n",
    "#general_model = tf.keras.models.load_model('bert_relation_extraction')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred = general_model.predict(X_test_input, batch_size=32, verbose=10)\n",
    "y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(y_test,y_pred_bool,target_names=['NoRelation','MinValue','MaxValue','IsValue']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
